{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import nest_asyncio\n",
    "from datasets import Dataset\n",
    "from IPython.display import Markdown\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "from src.settings import settings\n",
    "\n",
    "DATA = Path(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable async in jupyter notebook\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def gen_batches(iterable, n=100):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id d19dea99-70ed-4ffc-84a0-afbb06694bc1\n",
      "Started parsing the file under job_id 1c3cf003-573c-404a-987f-3f836b87ad42\n",
      ".............................................."
     ]
    }
   ],
   "source": [
    "# Parse documents using LlamaParse\n",
    "parser = LlamaParse(\n",
    "    api_key=settings.env.LLAMA_CLOUD_API_KEY,\n",
    "    result_type=\"markdown\",\n",
    "    verbose=True,\n",
    "    language=\"en\"\n",
    ")\n",
    "documents = await parser.aload_data(file_path=[str(DATA / f\"wellarchitected-framework-pt{i}.pdf\") for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into semantic chunks using OpenAI embeddings\n",
    "text_splitter = SemanticChunker(embeddings=OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=settings.env.OPENAI_API_KEY))\n",
    "splitted_documents = await text_splitter.atransform_documents(documents=[Document(page_content=doc.text) for doc in documents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out the first 2 chunks (table of content)\n",
    "new_splitted_documents = splitted_documents[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Generate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize gpt-4-turbo\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-4-turbo-preview\",\n",
    "    api_key=settings.env.OPENAI_API_KEY,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output pydantic parser\n",
    "class Questions(BaseModel):\n",
    "    \"\"\"Generated Synthetic Questions\"\"\"\n",
    "    questions: List[str] = Field(..., description=\"List of generated questions\")\n",
    "parser = PydanticOutputParser(pydantic_object=Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system message\n",
    "system = \"You are a synthetic question-answer pair generator. Given a chunk of context \" \\\n",
    "    \"about some topic(s), generate 3 example questions a user could ask and would\" \\\n",
    "    \" be answered using information from the chunk. For example, if the given con\" \\\n",
    "    \"text was a Wikipedia paragraph about the United States, an example question \" \\\n",
    "    \"could be 'How many states are in the United States?'. The questions should b\" \\\n",
    "    \"e able to be answered in a few words or less. Include only the questions in \" \\\n",
    "    \"your response.\\n{format_instructions}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain\n",
    "generate_instructions_prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{chunk}\")\n",
    "    ]\n",
    ")\n",
    "generate_instructions_chain = generate_instructions_prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format prompts for each chunk\n",
    "prompts = [\n",
    "    {\"chunk\": doc.page_content, \"format_instructions\": parser.get_format_instructions()}\n",
    "    for doc in new_splitted_documents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/6 processing...\n",
      "Batch 0/6 processed! sleeping...\n",
      "Batch 0/6 done!\n",
      "Batch 1/6 processing...\n",
      "Batch 1/6 processed! sleeping...\n",
      "Batch 1/6 done!\n",
      "Batch 2/6 processing...\n",
      "Batch 2/6 processed! sleeping...\n",
      "Batch 2/6 done!\n",
      "Batch 3/6 processing...\n",
      "Batch 3/6 processed! sleeping...\n",
      "Batch 3/6 done!\n",
      "Batch 4/6 processing...\n",
      "Batch 4/6 processed! sleeping...\n",
      "Batch 4/6 done!\n",
      "Batch 5/6 processing...\n",
      "Batch 5/6 processed! sleeping...\n",
      "Batch 5/6 done!\n"
     ]
    }
   ],
   "source": [
    "# Generate all the questions\n",
    "all_questions = []\n",
    "batch_size = 100\n",
    "total = len(prompts) // batch_size + 1\n",
    "\n",
    "for idx, batch in enumerate(gen_batches(prompts, batch_size)):\n",
    "        print(f\"Batch {idx}/{total} processing...\")\n",
    "        all_questions.extend(await generate_instructions_chain.abatch(batch, return_exceptions=False))\n",
    "        print(f\"Batch {idx}/{total} processed! sleeping...\")\n",
    "        await asyncio.sleep(60)\n",
    "        print(f\"Batch {idx}/{total} done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"questions\": [\n",
      "        \"What are the six pillars of the AWS Well-Architected Framework?\",\n",
      "        \"What does the term 'workload' refer to in the context of the AWS Well-Architected Framework?\",\n",
      "        \"What is the purpose of the AWS Well-Architected Tool?\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Inspect example\n",
    "print(all_questions[0].json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of expected questions: 1680\n",
      "Number of actual questions: 1671\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of expected questions: {len(all_questions) * 3}\")\n",
    "print(f\"Number of actual questions: {sum((len(q.questions) for q in all_questions))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save good cases (3 generated questions)\n",
    "_questions, _context = zip(*[(q, context) for q, context in zip(all_questions, new_splitted_documents) if len(q.questions) == 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Generate Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output pydantic parser\n",
    "class Answer(BaseModel):\n",
    "    \"\"\"Generated Chain-of-though Style Answer\"\"\"\n",
    "    answer: str = Field(..., description=\"Generated Chain-of-though answer\")\n",
    "parser = PydanticOutputParser(pydantic_object=Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system & user message\n",
    "system = \"You are a helpful question answerer who can provide an answer given a question and relevant context.\" \\\n",
    "\"Answer the question using the information given in the context. \" \\\n",
    "\"Here is things to pay attention to: \\n- First provide ste\" \\\n",
    "\"p-by-step reasoning on how to answer the question. \\n- In the re\" \\\n",
    "\"asoning, if you need to copy paste some sentences from the context, include \" \\\n",
    "\"them in ##begin_quote## and ##end_quote##. This would mean that things outsi\" \\\n",
    "\"de of ##begin_quote## and ##end_quote## are not directly copy paste from the\" \\\n",
    "\" context. \\n- End your response with final answer in the form <A\" \\\n",
    "\"NSWER>: $answer, the answer should be succint.\"\n",
    "user = \"#### Question: {question} \\n\\n#### Context: {context}\\n\\n{format_instructions}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define one shot\n",
    "os_question = \"The Oberoi family is part of a hotel company that has a head office in what city?\"\n",
    "os_context = \"[The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group]...[It is located in city center of Jakarta, near Mega Kuningan, adjacent to the sister JW Marriott Hotel. It is operated by The Ritz-Carlton Hotel Company. The complex has two towers that comprises a hotel and the Airlangga Apartment respectively]...[The Oberoi Group is a hotel company with its head office in Delhi.]\"\n",
    "os_user = f\"#### Question: {os_question} \\n\\n#### Context: {os_context}\" + \"\\n\\n{format_instructions}\"\n",
    "os_json_answer = \"\"\"```json\n",
    "{{\n",
    "    \"answer\": \"##Reason: The document ##begin_quote## The Oberoi family is an Indian family that is famous for its involvement in hotels, namely through The Oberoi Group. ##end_quote## establishes that the Oberoi family is involved in the Oberoi group, and the document ##begin_quote## The Oberoi Group is a hotel company with its head office in Delhi. ##end_quote## establishes the head office of The Oberoi Group. Therefore, the Oberoi family is part of a hotel company whose head office is in Delhi. ##Answer: Delhi\"\n",
    "}}\n",
    "```\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chain\n",
    "generate_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\", system),\n",
    "        (\"human\", os_user),\n",
    "        (\"ai\", os_json_answer),\n",
    "        (\"human\", user)\n",
    "    ]\n",
    ")\n",
    "generate_answer_chain = generate_answer_prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format prompts for each chunk\n",
    "prompts = []\n",
    "for i, doc in enumerate(_context):\n",
    "    for q in _questions[i].questions:\n",
    "        prompts.append({\n",
    "            \"question\": q,\n",
    "            \"context\": doc.page_content,\n",
    "            \"format_instructions\": parser.get_format_instructions()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/17 processing...\n",
      "Batch 1/17 processed! sleeping...\n",
      "Batch 1/17 done!\n",
      "Batch 2/17 processing...\n",
      "Batch 2/17 processed! sleeping...\n",
      "Batch 2/17 done!\n",
      "Batch 3/17 processing...\n",
      "Batch 3/17 processed! sleeping...\n",
      "Batch 3/17 done!\n",
      "Batch 4/17 processing...\n",
      "Batch 4/17 processed! sleeping...\n",
      "Batch 4/17 done!\n",
      "Batch 5/17 processing...\n",
      "Batch 5/17 processed! sleeping...\n",
      "Batch 5/17 done!\n",
      "Batch 6/17 processing...\n",
      "Batch 6/17 processed! sleeping...\n",
      "Batch 6/17 done!\n",
      "Batch 7/17 processing...\n",
      "Batch 7/17 processed! sleeping...\n",
      "Batch 7/17 done!\n",
      "Batch 8/17 processing...\n",
      "Batch 8/17 processed! sleeping...\n",
      "Batch 8/17 done!\n",
      "Batch 9/17 processing...\n",
      "Batch 9/17 processed! sleeping...\n",
      "Batch 9/17 done!\n",
      "Batch 10/17 processing...\n",
      "Batch 10/17 processed! sleeping...\n",
      "Batch 10/17 done!\n",
      "Batch 11/17 processing...\n",
      "Batch 11/17 processed! sleeping...\n",
      "Batch 11/17 done!\n",
      "Batch 12/17 processing...\n",
      "Batch 12/17 processed! sleeping...\n",
      "Batch 12/17 done!\n",
      "Batch 13/17 processing...\n",
      "Batch 13/17 processed! sleeping...\n",
      "Batch 13/17 done!\n",
      "Batch 14/17 processing...\n",
      "Batch 14/17 processed! sleeping...\n",
      "Batch 14/17 done!\n",
      "Batch 15/17 processing...\n",
      "Batch 15/17 processed! sleeping...\n",
      "Batch 15/17 done!\n",
      "Batch 16/17 processing...\n",
      "Batch 16/17 processed! sleeping...\n",
      "Batch 16/17 done!\n",
      "Batch 17/17 processing...\n",
      "Batch 17/17 processed! sleeping...\n",
      "Batch 17/17 done!\n"
     ]
    }
   ],
   "source": [
    "# Generate all the answers\n",
    "_answers = []\n",
    "batch_size = 100\n",
    "total = len(prompts) // batch_size + 1\n",
    "\n",
    "for idx, batch in enumerate(gen_batches(prompts, batch_size), start=1):\n",
    "        print(f\"Batch {idx}/{total} processing...\")\n",
    "        _answers.extend(await generate_answer_chain.abatch(batch, return_exceptions=False))\n",
    "        print(f\"Batch {idx}/{total} processed! sleeping...\")\n",
    "        await asyncio.sleep(60)\n",
    "        print(f\"Batch {idx}/{total} done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"answer\": \"##Reason: The AWS Well-Architected Framework is based on six pillars, as outlined in the context. These pillars are: ##begin_quote## operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability. ##end_quote## Each pillar focuses on a specific aspect of architecture best practices and strategies for designing and operating reliable, secure, efficient, cost-effective, and sustainable systems in the cloud. ##Answer: operational excellence, security, reliability, performance efficiency, cost optimization, sustainability\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Inspect example\n",
    "print(_answers[0].json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Flatten lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1671, 1671, 1671)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure that the lists have the same length for ease of the next steps\n",
    "_flattened_questions = [q for qs in _questions for q in qs.questions]\n",
    "_flattened_context = [chunk.page_content for chunk in _context for _ in range(3)]\n",
    "len(_flattened_context), len(_flattened_questions), len(_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Context"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You can script your operations procedures and automate their process by launching them in response to events. By performing operations as code, you limit human error and create consistent responses to events. - Make frequent, small, reversible changes: Design workloads that are scalable and loosely coupled to permit components to be updated regularly. Automated deployment techniques together with smaller, incremental changes reduces the blast radius and allows for faster reversal when failures occur. This increases confidence to deliver beneficial changes to your workload while maintaining quality and adapting quickly to changes in market conditions. - Refine operations procedures frequently: As you evolve your workloads, evolve your operations appropriately. As you use operations procedures, look for opportunities to improve them. Hold regular reviews and validate that all procedures are effective and that teams are familiar with them. Where gaps are identified, update procedures accordingly. Communicate procedural updates to all stakeholders and teams. Gamify your operations to share best practices and educate teams. - Anticipate failure: Perform “pre-mortem” exercises to identify potential sources of failure so that they can be removed or mitigated. Test your failure scenarios and validate your understanding of their impact. Test your response procedures to ensure they are effective and that teams are familiar with their process. Set up regular game days to test workload and team responses to simulated events. - Learn from all operational failures: Drive improvement through lessons learned from all operational events and failures."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Question"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "What is the benefit of making frequent, small, reversible changes in operations?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Answer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "##Reason: The context ##begin_quote## Make frequent, small, reversible changes: Design workloads that are scalable and loosely coupled to permit components to be updated regularly. Automated deployment techniques together with smaller, incremental changes reduces the blast radius and allows for faster reversal when failures occur. This increases confidence to deliver beneficial changes to your workload while maintaining quality and adapting quickly to changes in market conditions. ##end_quote## highlights the benefits of making frequent, small, reversible changes in operations. These benefits include reducing the blast radius of changes, allowing for faster reversal when failures occur, increasing confidence in delivering beneficial changes, maintaining quality, and adapting quickly to changes in market conditions. ##Answer: Reducing the blast radius of changes, allowing for faster reversal when failures occur, increasing confidence in delivering beneficial changes, maintaining quality, and adapting quickly to changes in market conditions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify index match\n",
    "display(Markdown(\"#### Context\"))\n",
    "display(Markdown(_flattened_context[10]))\n",
    "display(Markdown(\"#### Question\"))\n",
    "display(Markdown(_flattened_questions[10]))\n",
    "display(Markdown(\"#### Answer\"))\n",
    "display(Markdown(_answers[10].answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Populate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataset\n",
    "dataset = Dataset.from_dict({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_distract = 3 # Number of distractor documents\n",
    "p = 0.9 # Probability of including the oracle document\n",
    "\n",
    "for i, (question, chunk, answer) in enumerate(zip(_flattened_questions, _flattened_context, _answers)):\n",
    "    datapt = {\n",
    "        \"id\": f\"seed_task_{dataset.num_rows}\",\n",
    "        \"type\": \"general\",\n",
    "        \"question\": question,\n",
    "        \"context\": None,\n",
    "        \"oracle_context\": chunk,\n",
    "        \"cot_answer\": answer.answer,\n",
    "        \"instruction\": None\n",
    "    }\n",
    "\n",
    "    # Select distractor documents\n",
    "    distractor_indices = random.sample(population=[j for j in range(len(_flattened_context)) if j != i], k=num_distract)\n",
    "    distractor_docs = [_flattened_context[j] for j in distractor_indices]\n",
    "\n",
    "    # Create the docs list with the chunk (oracle document) at the beginning\n",
    "    docs = [chunk] + distractor_docs\n",
    "\n",
    "    # Decide whether to replace the oracle document with a random distractor document\n",
    "    # With a probability of 1 - p, the oracle document is replaced by a randomly selected distractor document.\n",
    "    if random.uniform(0, 1) >= p:\n",
    "        docs[0] = _flattened_context[random.choice(seq=distractor_indices)]\n",
    "    random.shuffle(docs)\n",
    "\n",
    "    datapt[\"context\"] = {\n",
    "        \"title\": [[\"placeholder_title\"] * (num_distract + 1)],\n",
    "        \"sentences\": [docs]\n",
    "    }\n",
    "\n",
    "    # Construct model instruction\n",
    "    datapt[\"instruction\"] = \"\\n\".join([f\"<DOCUMENT>{str(doc)}</DOCUMENT>\" for doc in docs]) + \"\\n\" + question\n",
    "\n",
    "    # Add the datapoint to the dataset\n",
    "    dataset = dataset.add_item(datapt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'seed_task_0',\n",
       " 'type': 'general',\n",
       " 'question': 'What are the six pillars of the AWS Well-Architected Framework?',\n",
       " 'context': {'sentences': [['All other trademarks not owned by Amazon are the property of their respective owners, who may or may not be affiliated with, connected to, or sponsored by Amazon. ---\\n|Content|Page Number|\\n|---|---|\\n|Abstract and introduction|1|\\n|Introduction|1|\\n|Definitions|2|\\n|On architecture|4|\\n|General design principles|6|\\n|The pillars of the framework|8|\\n|Operational excellence|8|\\n|Design principles|9|\\n|Definition|10|\\n|Best practices|10|\\n|Resources|19|\\n|Security|20|\\n|Design principles|20|\\n|Definition|21|\\n|Best practices|22|\\n|Resources|31|\\n|Reliability|31|\\n|Design principles|32|\\n|Definition|32|\\n|Best practices|33|\\n|Resources|38|\\n|Performance efficiency|38|\\n|Design principles|39|\\n|Definition|39|\\n|Best practices|40|\\n|Resources|45|\\n|Cost optimization|46|\\n|Design principles|46|\\n|Definition|47|\\n|Best practices|48|\\n|Resources|53|\\n|Sustainability|54|\\n|Design principles|54|\\n|Definition|55|\\n---\\n|Content|Page Number|\\n|---|---|\\n|Best practices|56|\\n|Resources|62|\\n|The review process|63|\\n|Conclusion|65|\\n|Contributors|66|\\n|Further reading|67|\\n|Document revisions|68|\\n|Appendix: Questions and best practices|71|\\n\\n|Operational excellence|71|\\n|---|---|\\n|Organization|71|\\n|Prepare|106|\\n|Operate|171|\\n|Evolve|204|\\n|Security|220|\\n|Security foundations|221|\\n|Identity and access management|239|\\n|Detection|289|\\n|Infrastructure protection|299|\\n|Data protection|317|\\n|Incident response|346|\\n|Application security|367|\\n|Reliability|385|\\n|Foundations|386|\\n|Workload architecture|424|\\n|Change management|466|\\n|Failure management|503|\\n|Performance efficiency|599|\\n|Architecture selection|599|\\n|Compute and hardware|613|\\n|Data management|628|\\n|Networking and content delivery|651|\\n|Process and culture|681|\\n|Cost optimization|695|\\n|Practice Cloud Financial Management|695|\\n|Expenditure and usage awareness|718|\\n|Cost-effective resources|757|\\n---\\n## AWS Well-Architected Framework\\n\\n|Manage demand and supply resources|795|\\n|---|---|\\n|Optimize over time|807|\\n\\n### Sustainability\\n\\n|Region selection|815|\\n|---|---|\\n|Alignment to demand|817|\\n|Software and architecture|829|\\n|Data|839|\\n|Hardware and services|858|\\n|Process and culture|867|\\n\\nNotices\\n\\nAWS Glossary\\n---\\n## AWS Well-Architected Framework\\n\\nPublication date: October 3, 2023 (Document revisions)\\n\\nThe AWS Well-Architected Framework helps you understand the pros and cons of decisions you make while building systems on AWS. By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, cost-effective, and sustainable systems in the cloud. ### Introduction\\n\\nThe AWS Well-Architected Framework helps you understand the pros and cons of decisions you make while building systems on AWS. Using the Framework helps you learn architectural best practices for designing and operating secure, reliable, efficient, cost-effective, and sustainable workloads in the AWS Cloud. It provides a way for you to consistently measure your architectures against best practices and identify areas for improvement. The process for reviewing an architecture is a constructive conversation about architectural decisions, and is not an audit mechanism. We believe that having well-architected systems greatly increases the likelihood of business success. AWS Solutions Architects have years of experience architecting solutions across a wide variety of business verticals and use cases. We have helped design and review thousands of customers’ architectures on AWS. From this experience, we have identified best practices and core strategies for architecting systems in the cloud. The AWS Well-Architected Framework documents a set of foundational questions that help you to understand if a specific architecture aligns well with cloud best practices. The framework provides a consistent approach to evaluating systems against the qualities you expect from modern cloud-based systems, and the remediation that would be required to achieve those qualities. As AWS continues to evolve, and we continue to learn more from working with our customers, we will continue to refine the definition of well-architected. This framework is intended for those in technology roles, such as chief technology officers (CTOs), architects, developers, and operations team members. It describes AWS best practices and strategies to use when designing and operating a cloud workload, and provides links to further implementation details and architectural patterns. For more information, see the AWS Well-Architected homepage. ### Introduction\\n\\n1\\n---\\n## AWS Well-Architected Framework\\n\\nAWS also provides a service for reviewing your workloads at no charge. The AWS Well-Architected Tool (AWS WA Tool) is a service in the cloud that provides a consistent process for you to review and measure your architecture using the AWS Well-Architected Framework. The AWS WA Tool provides recommendations for making your workloads more reliable, secure, efficient, and cost-effective. To help you apply best practices, we have created AWS Well-Architected Labs, which provides you with a repository of code and documentation to give you hands-on experience implementing best practices. We also have teamed up with select AWS Partner Network (APN) Partners, who are members of the AWS Well-Architected Partner program. These AWS Partners have deep AWS knowledge, and can help you review and improve your workloads. ### Definitions\\n\\nEvery day, experts at AWS assist customers in architecting systems to take advantage of best practices in the cloud. We work with you on making architectural trade-offs as your designs evolve. As you deploy these systems into live environments, we learn how well these systems perform and the consequences of those trade-offs. Based on what we have learned, we have created the AWS Well-Architected Framework, which provides a consistent set of best practices for customers and partners to evaluate architectures, and provides a set of questions you can use to evaluate how well an architecture is aligned to AWS best practices. The AWS Well-Architected Framework is based on six pillars — operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability. |Name|Description|\\n|---|---|\\n|Operational excellence|The ability to support development and run workloads effectively, gain insight into their operations, and to continuously improve supporting processes and procedures to deliver business value.|\\n|Security|The security pillar describes how to take advantage of cloud technologies to protect|\\n---\\n## AWS Well-Architected Framework\\n\\n|Name|Description|\\n|---|---|\\n|Security|data, systems, and assets in a way that can improve your security posture.|\\n|Reliability|The reliability pillar encompasses the ability of a workload to perform its intended function correctly and consistently when it’s expected to. This includes the ability to operate and test the workload through its total lifecycle. This paper provides in-depth, best practice guidance for implementing reliable workloads on AWS.|\\n|Performance efficiency|The ability to use computing resources efficiently to meet system requirements, and to maintain that efficiency as demand changes and technologies evolve.|\\n|Cost optimization|The ability to run systems to deliver business value at the lowest price point.|\\n|Sustainability|The ability to continually improve sustainability impacts by reducing energy consumption and increasing efficiency across all components of a workload by maximizing the benefits from the provisioned resources and minimizing the total resources required.|\\n\\nIn the AWS Well-Architected Framework, we use these terms:\\n\\n- A component is the code, configuration, and AWS Resources that together deliver against a requirement. A component is often the unit of technical ownership, and is decoupled from other components. - The term workload is used to identify a set of components that together deliver business value. A workload is usually the level of detail that business and technology leaders communicate about. ## Definitions\\n---\\n## AWS Well-Architected Framework\\n\\n• We think about architecture as being how components work together in a workload. How components communicate and interact is often the focus of architecture diagrams. • Milestones mark key changes in your architecture as it evolves throughout the product lifecycle (design, implementation, testing, go live, and in production). • Within an organization the technology portfolio is the collection of workloads that are required for the business to operate. • The level of effort is categorizing the amount of time, effort, and complexity a task requires for implementation.',\n",
       "    \"- You are only using x86 instances. - You specify one instance type in your Amazon EC2 Auto Scaling configuration. - You use AWS instances in a manner that they were not designed for (for example, you use compute-optimized instances for a memory-intensive workload). - You do not evaluate new instance types regularly. - You do not check recommendations from AWS rightsizing tools such as AWS Compute Optimizer. Benefits of establishing this best practice: By using energy-efficient and right-sized instances, you are able to greatly reduce the environmental impact and cost of your workload. Hardware and services\\n\\n860\\n---\\n## AWS Well-Architected Framework\\n\\nLevel of risk exposed if this best practice is not established: Medium\\n\\n### Implementation guidance\\n\\nUsing efficient instances in cloud workload is crucial for lower resource usage and cost-effectiveness. Continually monitor the release of new instance types and take advantage of energy efficiency improvements, including those instance types designed to support specific workloads such as machine learning training and inference, and video transcoding. ### Implementation steps\\n\\n- Learn and explore instance types which can lower your workload environmental impact. - Subscribe to What's New with AWS to be up-to-date with the latest AWS technologies and instances. - Learn about different AWS instance types. - Learn about AWS Graviton-based instances which offer the best performance per watt of energy use in Amazon EC2 by watching re:Invent 2020 - Deep dive on AWS Graviton2 processor-powered Amazon EC2 instances and Deep dive into AWS Graviton3 and Amazon EC2 C7g instances. - Plan and transition your workload to instance types with the least impact.\",\n",
       "    'AWS products or services are provided “as is” without warranties, representations, or conditions of any kind, whether express or implied. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. Copyright © 2023 Amazon Web Services, Inc.',\n",
       "    'For more detail, see the Logging AWS KMS API calls with CloudTrail section of the AWS KMS developers guide. ### Data protection\\n\\n327\\n---\\n## AWS Well-Architected Framework\\n\\n### Resources\\n\\nRelated documents:\\n\\n- AWS Key Management Service\\n- AWS cryptographic services and tools\\n- Protecting Amazon S3 Data Using Encryption\\n- Envelope encryption\\n- Digital sovereignty pledge\\n- Demystifying AWS KMS key operations, bring your own key, custom key store, and ciphertext portability\\n- AWS Key Management Service cryptographic details\\n\\nRelated videos:\\n\\n- How Encryption Works in AWS\\n- Securing Your Block Storage on AWS\\n- AWS data protection: Using locks, keys, signatures, and certificates\\n\\nRelated examples:\\n\\n- Implement advanced access control mechanisms using AWS KMS\\n\\n|SEC08-BP02|Enforce encryption at rest|\\n|---|---|\\n|You should enforce the use of encryption for data at rest. Encryption maintains the confidentiality of sensitive data in the event of unauthorized access or accidental disclosure.|You should enforce the use of encryption for data at rest. Encryption maintains the confidentiality of sensitive data in the event of unauthorized access or accidental disclosure.|\\n\\nDesired outcome: Private data should be encrypted by default when at rest. Encryption helps maintain confidentiality of the data and provides an additional layer of protection against intentional or inadvertent data disclosure or exfiltration. Data that is encrypted cannot be read or accessed without first unencrypting the data. Any data stored unencrypted should be inventoried and controlled. Common anti-patterns:\\n\\n- Not using encrypt-by-default configurations. Data protection\\n\\n328\\n---\\n## AWS Well-Architected Framework\\n\\nFramework\\n\\n- Providing overly permissive access to decryption keys. - Not monitoring the use of encryption and decryption keys. - Storing data unencrypted. - Using the same encryption key for all data regardless of data usage, types, and classification. Level of risk exposed if this best practice is not established: High\\n\\n### Implementation guidance\\n\\nMap encryption keys to data classifications within your workloads. This approach helps protect against overly permissive access when using either a single, or very small number of encryption keys for your data (see SEC07-BP01 Identify the data within your workload). AWS Key Management Service (AWS KMS) integrates with many AWS services to make it easier to encrypt your data at rest. For example, in Amazon Simple Storage Service (Amazon S3), you can set default encryption on a bucket so that new objects are automatically encrypted. When using AWS KMS, consider how tightly the data needs to be restricted. Default and service-controlled AWS KMS keys are managed and used on your behalf by AWS. For sensitive data that requires fine-grained access to the underlying encryption key, consider customer managed keys (CMKs). You have full control over CMKs, including rotation and access management through the use of key policies. Additionally, Amazon Elastic Compute Cloud (Amazon EC2) and Amazon S3 support the enforcement of encryption by setting default encryption. You can use AWS Config Rules to check automatically that you are using encryption, for example, for Amazon Elastic Block Store (Amazon EBS) volumes, Amazon Relational Database Service (Amazon RDS) instances, and Amazon S3 buckets. AWS also provides options for client-side encryption, allowing you to encrypt data prior to uploading it to the cloud. The AWS Encryption SDK provides a way to encrypt your data using envelope encryption. You provide the wrapping key, and the AWS Encryption SDK generates a unique data key for each data object it encrypts. Consider AWS CloudHSM if you need a managed single-tenant hardware security module (HSM). AWS CloudHSM allows you to generate, import, and manage cryptographic keys on a FIPS 140-2 level 3 validated HSM. Some use cases for AWS CloudHSM include protecting private keys for issuing a certificate authority (CA), and turning on transparent data encryption (TDE) for Oracle databases. The AWS CloudHSM Client SDK provides software that allows you to encrypt data client side using keys stored inside AWS CloudHSM prior to uploading your data into AWS. The Amazon DynamoDB Encryption Client also allows you to encrypt and sign items prior to upload into a DynamoDB table. Data protection\\n---\\n## AWS Well-Architected Framework\\n\\nImplementation steps\\n\\n- Enforce encryption at rest for Amazon S3: Implement Amazon S3 bucket default encryption. - Configure default encryption for new Amazon EBS volumes: Specify that you want all newly created Amazon EBS volumes to be created in encrypted form, with the option of using the default key provided by AWS or a key that you create. - Configure encrypted Amazon Machine Images (AMIs): Copying an existing AMI with encryption configured will automatically encrypt root volumes and snapshots. - Configure Amazon RDS encryption: Configure encryption for your Amazon RDS database clusters and snapshots at rest by using the encryption option. - Create and configure AWS KMS keys with policies that limit access to the appropriate principals for each classification of data: For example, create one AWS KMS key for encrypting production data and a different key for encrypting development or test data. You can also provide key access to other AWS accounts. Consider having different accounts for your development and production environments. If your production environment needs to decrypt artifacts in the development account, you can edit the CMK policy used to encrypt the development artifacts to give the production account the ability to decrypt those artifacts. The production environment can then ingest the decrypted data for use in production. - Configure encryption in additional AWS services: For other AWS services you use, review the security documentation for that service to determine the service’s encryption options. Resources\\n\\nRelated documents:\\n\\n|AWS Crypto Tools|330|\\n|---|---|\\n|AWS Encryption SDK|330|\\n|AWS KMS Cryptographic Details Whitepaper|330|\\n|AWS Key Management Service|330|\\n|AWS cryptographic services and tools|330|\\n|Amazon EBS Encryption|330|\\n|Default encryption for Amazon EBS volumes|330|\\n|Encrypting Amazon RDS Resources|330|\\n\\nData protection\\n---\\n## AWS Well-Architected Framework\\n\\nFramework\\n\\n- How do I enable default encryption for an Amazon S3 bucket? - Protecting Amazon S3 Data Using Encryption\\n\\nRelated videos:\\n\\n- How Encryption Works in AWS\\n- Securing Your Block Storage on AWS\\n\\nSEC08-BP03 Automate data at rest protection\\n\\nUse automated tools to validate and enforce data at rest controls continuously, for example, verify that there are only encrypted storage resources. You can automate validation that all EBS volumes are encrypted using AWS Config Rules. AWS Security Hub can also verify several different controls through automated checks against security standards. Additionally, your AWS Config Rules can automatically remediate noncompliant resources. Level of risk exposed if this best practice is not established: Medium\\n\\nImplementation guidance\\n\\nData at rest represents any data that you persist in non-volatile storage for any duration in your workload. This includes block storage, object storage, databases, archives, IoT devices, and any other storage medium on which data is persisted. Protecting your data at rest reduces the risk of unauthorized access, when encryption and appropriate access controls are implemented. Enforce encryption at rest: You should ensure that the only way to store data is by using encryption. AWS KMS integrates seamlessly with many AWS services to make it easier for you to encrypt all your data at rest. For example, in Amazon Simple Storage Service (Amazon S3) you can set default encryption on a bucket so that all new objects are automatically encrypted. Additionally, Amazon EC2 and Amazon S3 support the enforcement of encryption by setting default encryption. You can use AWS Managed Config Rules to check automatically that you are using encryption, for example, for EBS volumes, Amazon Relational Database Service (Amazon RDS) instances, and Amazon S3 buckets. Resources\\n\\nRelated documents:\\n\\n- AWS Crypto Tools\\n\\nData protection\\n\\n331\\n---\\n## AWS Well-Architected Framework\\n\\nFramework\\n\\n- AWS Encryption SDK\\n\\nRelated videos:\\n\\n- How Encryption Works in AWS\\n- Securing Your Block Storage on AWS\\n\\nSEC08-BP04 Enforce access control\\n\\nTo help protect your data at rest, enforce access control using mechanisms, such as isolation and versioning, and apply the principle of least privilege. Prevent the granting of public access to your data.']],\n",
       "  'title': [['placeholder_title',\n",
       "    'placeholder_title',\n",
       "    'placeholder_title',\n",
       "    'placeholder_title']]},\n",
       " 'oracle_context': 'All other trademarks not owned by Amazon are the property of their respective owners, who may or may not be affiliated with, connected to, or sponsored by Amazon. ---\\n|Content|Page Number|\\n|---|---|\\n|Abstract and introduction|1|\\n|Introduction|1|\\n|Definitions|2|\\n|On architecture|4|\\n|General design principles|6|\\n|The pillars of the framework|8|\\n|Operational excellence|8|\\n|Design principles|9|\\n|Definition|10|\\n|Best practices|10|\\n|Resources|19|\\n|Security|20|\\n|Design principles|20|\\n|Definition|21|\\n|Best practices|22|\\n|Resources|31|\\n|Reliability|31|\\n|Design principles|32|\\n|Definition|32|\\n|Best practices|33|\\n|Resources|38|\\n|Performance efficiency|38|\\n|Design principles|39|\\n|Definition|39|\\n|Best practices|40|\\n|Resources|45|\\n|Cost optimization|46|\\n|Design principles|46|\\n|Definition|47|\\n|Best practices|48|\\n|Resources|53|\\n|Sustainability|54|\\n|Design principles|54|\\n|Definition|55|\\n---\\n|Content|Page Number|\\n|---|---|\\n|Best practices|56|\\n|Resources|62|\\n|The review process|63|\\n|Conclusion|65|\\n|Contributors|66|\\n|Further reading|67|\\n|Document revisions|68|\\n|Appendix: Questions and best practices|71|\\n\\n|Operational excellence|71|\\n|---|---|\\n|Organization|71|\\n|Prepare|106|\\n|Operate|171|\\n|Evolve|204|\\n|Security|220|\\n|Security foundations|221|\\n|Identity and access management|239|\\n|Detection|289|\\n|Infrastructure protection|299|\\n|Data protection|317|\\n|Incident response|346|\\n|Application security|367|\\n|Reliability|385|\\n|Foundations|386|\\n|Workload architecture|424|\\n|Change management|466|\\n|Failure management|503|\\n|Performance efficiency|599|\\n|Architecture selection|599|\\n|Compute and hardware|613|\\n|Data management|628|\\n|Networking and content delivery|651|\\n|Process and culture|681|\\n|Cost optimization|695|\\n|Practice Cloud Financial Management|695|\\n|Expenditure and usage awareness|718|\\n|Cost-effective resources|757|\\n---\\n## AWS Well-Architected Framework\\n\\n|Manage demand and supply resources|795|\\n|---|---|\\n|Optimize over time|807|\\n\\n### Sustainability\\n\\n|Region selection|815|\\n|---|---|\\n|Alignment to demand|817|\\n|Software and architecture|829|\\n|Data|839|\\n|Hardware and services|858|\\n|Process and culture|867|\\n\\nNotices\\n\\nAWS Glossary\\n---\\n## AWS Well-Architected Framework\\n\\nPublication date: October 3, 2023 (Document revisions)\\n\\nThe AWS Well-Architected Framework helps you understand the pros and cons of decisions you make while building systems on AWS. By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, cost-effective, and sustainable systems in the cloud. ### Introduction\\n\\nThe AWS Well-Architected Framework helps you understand the pros and cons of decisions you make while building systems on AWS. Using the Framework helps you learn architectural best practices for designing and operating secure, reliable, efficient, cost-effective, and sustainable workloads in the AWS Cloud. It provides a way for you to consistently measure your architectures against best practices and identify areas for improvement. The process for reviewing an architecture is a constructive conversation about architectural decisions, and is not an audit mechanism. We believe that having well-architected systems greatly increases the likelihood of business success. AWS Solutions Architects have years of experience architecting solutions across a wide variety of business verticals and use cases. We have helped design and review thousands of customers’ architectures on AWS. From this experience, we have identified best practices and core strategies for architecting systems in the cloud. The AWS Well-Architected Framework documents a set of foundational questions that help you to understand if a specific architecture aligns well with cloud best practices. The framework provides a consistent approach to evaluating systems against the qualities you expect from modern cloud-based systems, and the remediation that would be required to achieve those qualities. As AWS continues to evolve, and we continue to learn more from working with our customers, we will continue to refine the definition of well-architected. This framework is intended for those in technology roles, such as chief technology officers (CTOs), architects, developers, and operations team members. It describes AWS best practices and strategies to use when designing and operating a cloud workload, and provides links to further implementation details and architectural patterns. For more information, see the AWS Well-Architected homepage. ### Introduction\\n\\n1\\n---\\n## AWS Well-Architected Framework\\n\\nAWS also provides a service for reviewing your workloads at no charge. The AWS Well-Architected Tool (AWS WA Tool) is a service in the cloud that provides a consistent process for you to review and measure your architecture using the AWS Well-Architected Framework. The AWS WA Tool provides recommendations for making your workloads more reliable, secure, efficient, and cost-effective. To help you apply best practices, we have created AWS Well-Architected Labs, which provides you with a repository of code and documentation to give you hands-on experience implementing best practices. We also have teamed up with select AWS Partner Network (APN) Partners, who are members of the AWS Well-Architected Partner program. These AWS Partners have deep AWS knowledge, and can help you review and improve your workloads. ### Definitions\\n\\nEvery day, experts at AWS assist customers in architecting systems to take advantage of best practices in the cloud. We work with you on making architectural trade-offs as your designs evolve. As you deploy these systems into live environments, we learn how well these systems perform and the consequences of those trade-offs. Based on what we have learned, we have created the AWS Well-Architected Framework, which provides a consistent set of best practices for customers and partners to evaluate architectures, and provides a set of questions you can use to evaluate how well an architecture is aligned to AWS best practices. The AWS Well-Architected Framework is based on six pillars — operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability. |Name|Description|\\n|---|---|\\n|Operational excellence|The ability to support development and run workloads effectively, gain insight into their operations, and to continuously improve supporting processes and procedures to deliver business value.|\\n|Security|The security pillar describes how to take advantage of cloud technologies to protect|\\n---\\n## AWS Well-Architected Framework\\n\\n|Name|Description|\\n|---|---|\\n|Security|data, systems, and assets in a way that can improve your security posture.|\\n|Reliability|The reliability pillar encompasses the ability of a workload to perform its intended function correctly and consistently when it’s expected to. This includes the ability to operate and test the workload through its total lifecycle. This paper provides in-depth, best practice guidance for implementing reliable workloads on AWS.|\\n|Performance efficiency|The ability to use computing resources efficiently to meet system requirements, and to maintain that efficiency as demand changes and technologies evolve.|\\n|Cost optimization|The ability to run systems to deliver business value at the lowest price point.|\\n|Sustainability|The ability to continually improve sustainability impacts by reducing energy consumption and increasing efficiency across all components of a workload by maximizing the benefits from the provisioned resources and minimizing the total resources required.|\\n\\nIn the AWS Well-Architected Framework, we use these terms:\\n\\n- A component is the code, configuration, and AWS Resources that together deliver against a requirement. A component is often the unit of technical ownership, and is decoupled from other components. - The term workload is used to identify a set of components that together deliver business value. A workload is usually the level of detail that business and technology leaders communicate about. ## Definitions\\n---\\n## AWS Well-Architected Framework\\n\\n• We think about architecture as being how components work together in a workload. How components communicate and interact is often the focus of architecture diagrams. • Milestones mark key changes in your architecture as it evolves throughout the product lifecycle (design, implementation, testing, go live, and in production). • Within an organization the technology portfolio is the collection of workloads that are required for the business to operate. • The level of effort is categorizing the amount of time, effort, and complexity a task requires for implementation.',\n",
       " 'cot_answer': '##Reason: The AWS Well-Architected Framework is based on six pillars, as outlined in the context. These pillars are: ##begin_quote## operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability. ##end_quote## Each pillar focuses on a specific aspect of architecture best practices and strategies for designing and operating reliable, secure, efficient, cost-effective, and sustainable systems in the cloud. ##Answer: operational excellence, security, reliability, performance efficiency, cost optimization, sustainability',\n",
       " 'instruction': \"<DOCUMENT>All other trademarks not owned by Amazon are the property of their respective owners, who may or may not be affiliated with, connected to, or sponsored by Amazon. ---\\n|Content|Page Number|\\n|---|---|\\n|Abstract and introduction|1|\\n|Introduction|1|\\n|Definitions|2|\\n|On architecture|4|\\n|General design principles|6|\\n|The pillars of the framework|8|\\n|Operational excellence|8|\\n|Design principles|9|\\n|Definition|10|\\n|Best practices|10|\\n|Resources|19|\\n|Security|20|\\n|Design principles|20|\\n|Definition|21|\\n|Best practices|22|\\n|Resources|31|\\n|Reliability|31|\\n|Design principles|32|\\n|Definition|32|\\n|Best practices|33|\\n|Resources|38|\\n|Performance efficiency|38|\\n|Design principles|39|\\n|Definition|39|\\n|Best practices|40|\\n|Resources|45|\\n|Cost optimization|46|\\n|Design principles|46|\\n|Definition|47|\\n|Best practices|48|\\n|Resources|53|\\n|Sustainability|54|\\n|Design principles|54|\\n|Definition|55|\\n---\\n|Content|Page Number|\\n|---|---|\\n|Best practices|56|\\n|Resources|62|\\n|The review process|63|\\n|Conclusion|65|\\n|Contributors|66|\\n|Further reading|67|\\n|Document revisions|68|\\n|Appendix: Questions and best practices|71|\\n\\n|Operational excellence|71|\\n|---|---|\\n|Organization|71|\\n|Prepare|106|\\n|Operate|171|\\n|Evolve|204|\\n|Security|220|\\n|Security foundations|221|\\n|Identity and access management|239|\\n|Detection|289|\\n|Infrastructure protection|299|\\n|Data protection|317|\\n|Incident response|346|\\n|Application security|367|\\n|Reliability|385|\\n|Foundations|386|\\n|Workload architecture|424|\\n|Change management|466|\\n|Failure management|503|\\n|Performance efficiency|599|\\n|Architecture selection|599|\\n|Compute and hardware|613|\\n|Data management|628|\\n|Networking and content delivery|651|\\n|Process and culture|681|\\n|Cost optimization|695|\\n|Practice Cloud Financial Management|695|\\n|Expenditure and usage awareness|718|\\n|Cost-effective resources|757|\\n---\\n## AWS Well-Architected Framework\\n\\n|Manage demand and supply resources|795|\\n|---|---|\\n|Optimize over time|807|\\n\\n### Sustainability\\n\\n|Region selection|815|\\n|---|---|\\n|Alignment to demand|817|\\n|Software and architecture|829|\\n|Data|839|\\n|Hardware and services|858|\\n|Process and culture|867|\\n\\nNotices\\n\\nAWS Glossary\\n---\\n## AWS Well-Architected Framework\\n\\nPublication date: October 3, 2023 (Document revisions)\\n\\nThe AWS Well-Architected Framework helps you understand the pros and cons of decisions you make while building systems on AWS. By using the Framework you will learn architectural best practices for designing and operating reliable, secure, efficient, cost-effective, and sustainable systems in the cloud. ### Introduction\\n\\nThe AWS Well-Architected Framework helps you understand the pros and cons of decisions you make while building systems on AWS. Using the Framework helps you learn architectural best practices for designing and operating secure, reliable, efficient, cost-effective, and sustainable workloads in the AWS Cloud. It provides a way for you to consistently measure your architectures against best practices and identify areas for improvement. The process for reviewing an architecture is a constructive conversation about architectural decisions, and is not an audit mechanism. We believe that having well-architected systems greatly increases the likelihood of business success. AWS Solutions Architects have years of experience architecting solutions across a wide variety of business verticals and use cases. We have helped design and review thousands of customers’ architectures on AWS. From this experience, we have identified best practices and core strategies for architecting systems in the cloud. The AWS Well-Architected Framework documents a set of foundational questions that help you to understand if a specific architecture aligns well with cloud best practices. The framework provides a consistent approach to evaluating systems against the qualities you expect from modern cloud-based systems, and the remediation that would be required to achieve those qualities. As AWS continues to evolve, and we continue to learn more from working with our customers, we will continue to refine the definition of well-architected. This framework is intended for those in technology roles, such as chief technology officers (CTOs), architects, developers, and operations team members. It describes AWS best practices and strategies to use when designing and operating a cloud workload, and provides links to further implementation details and architectural patterns. For more information, see the AWS Well-Architected homepage. ### Introduction\\n\\n1\\n---\\n## AWS Well-Architected Framework\\n\\nAWS also provides a service for reviewing your workloads at no charge. The AWS Well-Architected Tool (AWS WA Tool) is a service in the cloud that provides a consistent process for you to review and measure your architecture using the AWS Well-Architected Framework. The AWS WA Tool provides recommendations for making your workloads more reliable, secure, efficient, and cost-effective. To help you apply best practices, we have created AWS Well-Architected Labs, which provides you with a repository of code and documentation to give you hands-on experience implementing best practices. We also have teamed up with select AWS Partner Network (APN) Partners, who are members of the AWS Well-Architected Partner program. These AWS Partners have deep AWS knowledge, and can help you review and improve your workloads. ### Definitions\\n\\nEvery day, experts at AWS assist customers in architecting systems to take advantage of best practices in the cloud. We work with you on making architectural trade-offs as your designs evolve. As you deploy these systems into live environments, we learn how well these systems perform and the consequences of those trade-offs. Based on what we have learned, we have created the AWS Well-Architected Framework, which provides a consistent set of best practices for customers and partners to evaluate architectures, and provides a set of questions you can use to evaluate how well an architecture is aligned to AWS best practices. The AWS Well-Architected Framework is based on six pillars — operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability. |Name|Description|\\n|---|---|\\n|Operational excellence|The ability to support development and run workloads effectively, gain insight into their operations, and to continuously improve supporting processes and procedures to deliver business value.|\\n|Security|The security pillar describes how to take advantage of cloud technologies to protect|\\n---\\n## AWS Well-Architected Framework\\n\\n|Name|Description|\\n|---|---|\\n|Security|data, systems, and assets in a way that can improve your security posture.|\\n|Reliability|The reliability pillar encompasses the ability of a workload to perform its intended function correctly and consistently when it’s expected to. This includes the ability to operate and test the workload through its total lifecycle. This paper provides in-depth, best practice guidance for implementing reliable workloads on AWS.|\\n|Performance efficiency|The ability to use computing resources efficiently to meet system requirements, and to maintain that efficiency as demand changes and technologies evolve.|\\n|Cost optimization|The ability to run systems to deliver business value at the lowest price point.|\\n|Sustainability|The ability to continually improve sustainability impacts by reducing energy consumption and increasing efficiency across all components of a workload by maximizing the benefits from the provisioned resources and minimizing the total resources required.|\\n\\nIn the AWS Well-Architected Framework, we use these terms:\\n\\n- A component is the code, configuration, and AWS Resources that together deliver against a requirement. A component is often the unit of technical ownership, and is decoupled from other components. - The term workload is used to identify a set of components that together deliver business value. A workload is usually the level of detail that business and technology leaders communicate about. ## Definitions\\n---\\n## AWS Well-Architected Framework\\n\\n• We think about architecture as being how components work together in a workload. How components communicate and interact is often the focus of architecture diagrams. • Milestones mark key changes in your architecture as it evolves throughout the product lifecycle (design, implementation, testing, go live, and in production). • Within an organization the technology portfolio is the collection of workloads that are required for the business to operate. • The level of effort is categorizing the amount of time, effort, and complexity a task requires for implementation.</DOCUMENT>\\n<DOCUMENT>- You are only using x86 instances. - You specify one instance type in your Amazon EC2 Auto Scaling configuration. - You use AWS instances in a manner that they were not designed for (for example, you use compute-optimized instances for a memory-intensive workload). - You do not evaluate new instance types regularly. - You do not check recommendations from AWS rightsizing tools such as AWS Compute Optimizer. Benefits of establishing this best practice: By using energy-efficient and right-sized instances, you are able to greatly reduce the environmental impact and cost of your workload. Hardware and services\\n\\n860\\n---\\n## AWS Well-Architected Framework\\n\\nLevel of risk exposed if this best practice is not established: Medium\\n\\n### Implementation guidance\\n\\nUsing efficient instances in cloud workload is crucial for lower resource usage and cost-effectiveness. Continually monitor the release of new instance types and take advantage of energy efficiency improvements, including those instance types designed to support specific workloads such as machine learning training and inference, and video transcoding. ### Implementation steps\\n\\n- Learn and explore instance types which can lower your workload environmental impact. - Subscribe to What's New with AWS to be up-to-date with the latest AWS technologies and instances. - Learn about different AWS instance types. - Learn about AWS Graviton-based instances which offer the best performance per watt of energy use in Amazon EC2 by watching re:Invent 2020 - Deep dive on AWS Graviton2 processor-powered Amazon EC2 instances and Deep dive into AWS Graviton3 and Amazon EC2 C7g instances. - Plan and transition your workload to instance types with the least impact.</DOCUMENT>\\n<DOCUMENT>AWS products or services are provided “as is” without warranties, representations, or conditions of any kind, whether express or implied. The responsibilities and liabilities of AWS to its customers are controlled by AWS agreements, and this document is not part of, nor does it modify, any agreement between AWS and its customers. Copyright © 2023 Amazon Web Services, Inc.</DOCUMENT>\\n<DOCUMENT>For more detail, see the Logging AWS KMS API calls with CloudTrail section of the AWS KMS developers guide. ### Data protection\\n\\n327\\n---\\n## AWS Well-Architected Framework\\n\\n### Resources\\n\\nRelated documents:\\n\\n- AWS Key Management Service\\n- AWS cryptographic services and tools\\n- Protecting Amazon S3 Data Using Encryption\\n- Envelope encryption\\n- Digital sovereignty pledge\\n- Demystifying AWS KMS key operations, bring your own key, custom key store, and ciphertext portability\\n- AWS Key Management Service cryptographic details\\n\\nRelated videos:\\n\\n- How Encryption Works in AWS\\n- Securing Your Block Storage on AWS\\n- AWS data protection: Using locks, keys, signatures, and certificates\\n\\nRelated examples:\\n\\n- Implement advanced access control mechanisms using AWS KMS\\n\\n|SEC08-BP02|Enforce encryption at rest|\\n|---|---|\\n|You should enforce the use of encryption for data at rest. Encryption maintains the confidentiality of sensitive data in the event of unauthorized access or accidental disclosure.|You should enforce the use of encryption for data at rest. Encryption maintains the confidentiality of sensitive data in the event of unauthorized access or accidental disclosure.|\\n\\nDesired outcome: Private data should be encrypted by default when at rest. Encryption helps maintain confidentiality of the data and provides an additional layer of protection against intentional or inadvertent data disclosure or exfiltration. Data that is encrypted cannot be read or accessed without first unencrypting the data. Any data stored unencrypted should be inventoried and controlled. Common anti-patterns:\\n\\n- Not using encrypt-by-default configurations. Data protection\\n\\n328\\n---\\n## AWS Well-Architected Framework\\n\\nFramework\\n\\n- Providing overly permissive access to decryption keys. - Not monitoring the use of encryption and decryption keys. - Storing data unencrypted. - Using the same encryption key for all data regardless of data usage, types, and classification. Level of risk exposed if this best practice is not established: High\\n\\n### Implementation guidance\\n\\nMap encryption keys to data classifications within your workloads. This approach helps protect against overly permissive access when using either a single, or very small number of encryption keys for your data (see SEC07-BP01 Identify the data within your workload). AWS Key Management Service (AWS KMS) integrates with many AWS services to make it easier to encrypt your data at rest. For example, in Amazon Simple Storage Service (Amazon S3), you can set default encryption on a bucket so that new objects are automatically encrypted. When using AWS KMS, consider how tightly the data needs to be restricted. Default and service-controlled AWS KMS keys are managed and used on your behalf by AWS. For sensitive data that requires fine-grained access to the underlying encryption key, consider customer managed keys (CMKs). You have full control over CMKs, including rotation and access management through the use of key policies. Additionally, Amazon Elastic Compute Cloud (Amazon EC2) and Amazon S3 support the enforcement of encryption by setting default encryption. You can use AWS Config Rules to check automatically that you are using encryption, for example, for Amazon Elastic Block Store (Amazon EBS) volumes, Amazon Relational Database Service (Amazon RDS) instances, and Amazon S3 buckets. AWS also provides options for client-side encryption, allowing you to encrypt data prior to uploading it to the cloud. The AWS Encryption SDK provides a way to encrypt your data using envelope encryption. You provide the wrapping key, and the AWS Encryption SDK generates a unique data key for each data object it encrypts. Consider AWS CloudHSM if you need a managed single-tenant hardware security module (HSM). AWS CloudHSM allows you to generate, import, and manage cryptographic keys on a FIPS 140-2 level 3 validated HSM. Some use cases for AWS CloudHSM include protecting private keys for issuing a certificate authority (CA), and turning on transparent data encryption (TDE) for Oracle databases. The AWS CloudHSM Client SDK provides software that allows you to encrypt data client side using keys stored inside AWS CloudHSM prior to uploading your data into AWS. The Amazon DynamoDB Encryption Client also allows you to encrypt and sign items prior to upload into a DynamoDB table. Data protection\\n---\\n## AWS Well-Architected Framework\\n\\nImplementation steps\\n\\n- Enforce encryption at rest for Amazon S3: Implement Amazon S3 bucket default encryption. - Configure default encryption for new Amazon EBS volumes: Specify that you want all newly created Amazon EBS volumes to be created in encrypted form, with the option of using the default key provided by AWS or a key that you create. - Configure encrypted Amazon Machine Images (AMIs): Copying an existing AMI with encryption configured will automatically encrypt root volumes and snapshots. - Configure Amazon RDS encryption: Configure encryption for your Amazon RDS database clusters and snapshots at rest by using the encryption option. - Create and configure AWS KMS keys with policies that limit access to the appropriate principals for each classification of data: For example, create one AWS KMS key for encrypting production data and a different key for encrypting development or test data. You can also provide key access to other AWS accounts. Consider having different accounts for your development and production environments. If your production environment needs to decrypt artifacts in the development account, you can edit the CMK policy used to encrypt the development artifacts to give the production account the ability to decrypt those artifacts. The production environment can then ingest the decrypted data for use in production. - Configure encryption in additional AWS services: For other AWS services you use, review the security documentation for that service to determine the service’s encryption options. Resources\\n\\nRelated documents:\\n\\n|AWS Crypto Tools|330|\\n|---|---|\\n|AWS Encryption SDK|330|\\n|AWS KMS Cryptographic Details Whitepaper|330|\\n|AWS Key Management Service|330|\\n|AWS cryptographic services and tools|330|\\n|Amazon EBS Encryption|330|\\n|Default encryption for Amazon EBS volumes|330|\\n|Encrypting Amazon RDS Resources|330|\\n\\nData protection\\n---\\n## AWS Well-Architected Framework\\n\\nFramework\\n\\n- How do I enable default encryption for an Amazon S3 bucket? - Protecting Amazon S3 Data Using Encryption\\n\\nRelated videos:\\n\\n- How Encryption Works in AWS\\n- Securing Your Block Storage on AWS\\n\\nSEC08-BP03 Automate data at rest protection\\n\\nUse automated tools to validate and enforce data at rest controls continuously, for example, verify that there are only encrypted storage resources. You can automate validation that all EBS volumes are encrypted using AWS Config Rules. AWS Security Hub can also verify several different controls through automated checks against security standards. Additionally, your AWS Config Rules can automatically remediate noncompliant resources. Level of risk exposed if this best practice is not established: Medium\\n\\nImplementation guidance\\n\\nData at rest represents any data that you persist in non-volatile storage for any duration in your workload. This includes block storage, object storage, databases, archives, IoT devices, and any other storage medium on which data is persisted. Protecting your data at rest reduces the risk of unauthorized access, when encryption and appropriate access controls are implemented. Enforce encryption at rest: You should ensure that the only way to store data is by using encryption. AWS KMS integrates seamlessly with many AWS services to make it easier for you to encrypt all your data at rest. For example, in Amazon Simple Storage Service (Amazon S3) you can set default encryption on a bucket so that all new objects are automatically encrypted. Additionally, Amazon EC2 and Amazon S3 support the enforcement of encryption by setting default encryption. You can use AWS Managed Config Rules to check automatically that you are using encryption, for example, for EBS volumes, Amazon Relational Database Service (Amazon RDS) instances, and Amazon S3 buckets. Resources\\n\\nRelated documents:\\n\\n- AWS Crypto Tools\\n\\nData protection\\n\\n331\\n---\\n## AWS Well-Architected Framework\\n\\nFramework\\n\\n- AWS Encryption SDK\\n\\nRelated videos:\\n\\n- How Encryption Works in AWS\\n- Securing Your Block Storage on AWS\\n\\nSEC08-BP04 Enforce access control\\n\\nTo help protect your data at rest, enforce access control using mechanisms, such as isolation and versioning, and apply the principle of least privilege. Prevent the granting of public access to your data.</DOCUMENT>\\nWhat are the six pillars of the AWS Well-Architected Framework?\"}"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 12.43ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.85s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/jjovalle99/raft-dataset-aws-wellarchitected/commit/73fa19d70efe7429de30d8870c36b7d545186b2a', commit_message='Upload dataset', commit_description='', oid='73fa19d70efe7429de30d8870c36b7d545186b2a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"raft-dataset-aws-wellarchitected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![evid](assets/dataset-hf.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
